---
layout: post
title: 人工智能期末复习
date: 2019-12-26
categories: AI
tags: AI
---
[TOC]
# 绪论
## 四大流派
1. 符号主义（Symbolism）例：知识图谱
2. 连接主义（Connectionism）例：深度神经网络
3. 行为主义（Behaviourism）例：机器人
4. 统计主义（Statisticsism）例：机器学习

## AI 机器学习深度学习三者之间的异同和关联

包含关系

![image](https://note.youdao.com/yws/res/34619/58847E067EE449928A278768BC4CB80F)

- 人工智能，更宽泛的概念
- 机器学习 一种实现人工智能的方法，前面有特征维度 降维等人工特征提取，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。
- 深度学习  一种实现机器学习的技术 关注神经网络结构，一般端对端，关注输入和输出，数据清洗、格式转换、特征提取（卷积），然后喂入神经网络。

## 机器学习分为两个阶段

![image](https://note.youdao.com/yws/res/34607/05004533A6484FC4882CB92D6B66FD29)

### 训练： 三部曲 on Training set
- 定义Model
- 定义Loss function
- 反向传播，找到Model中最佳function，梯度下降迭代调整参数，是Loss function 最小
    epoch  batch_size  batch iteration:可以是epoch 的次数，可以是第几次参数更新，具体看代码。
- 找到参数
### 预测： on Dev Set/ testing set
- 前向传播去预测输出，使用开发集，计算Loss
- 用开发集去找到最佳的超参数

Training Set 和Testing Set一般 8 2 分； 也有交叉验证的技术

## 机器学习的分类
- 监督学习 标签，知道正确答案
- 非监督学习，无标签
- 半非监督学习
- 强化学习，激励
- 迁移学习

### 监督学习
监督学习的目标是建立一个学习过程，将预测结果与“训练数据”（即输入数据）的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。

我的理解是，知道正确答案（标签），数据集中的数据每一个都已经被标注了什么是正确的答案，都是代表签的数据。 例如肿瘤数据，带上了良性还是恶性的标签。

例：手写字符识别、肿瘤分类、预测天气、支持向量机、线性判别。

特点：训练样本数据和待分类的类别已知，且训练样本数据皆为标签数据。

### 非监督学习
非监督学习从无标记的训练数据中推断结论，它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。

我的理解是，没有标签，只有数据，不知道数据中的结构，

例：聚类分析、主成分分析。

特点：**训练样本数据和待分类的类别已知，但训练样本数据皆为非标签数据**。

### 半监督学习
半监督学习的训练数据通常是少量有标记数据及大量未标记数据，介于无监督学习和监督学习之间。

我的理解是，既有带标签的数据，又有无标签数据。

例：聚类假设、流形假设。

特点：训练样本数据和待分类的类别已知，然而训练样本既有标签数据，也有非标签数据。

### 强化学习
强化学习的输入数据作为对模型的反馈，强调如何基于环境而行动，以取得最大化的预期利益。

与监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。

我的理解是，当前环境的激励去影响下一次的行动。

例：学习下围棋、打星际争霸、DotA2、双人德州扑克。（全部都是1v1的场景，目前强化学习领域对于多人博弈研究的很少）

特点：决策流程，激励系统，学习一系列的行动。

### 迁移学习
迁移学习是通过从已学习的相关任务中迁移其知识来对需要学习的新任务进行提高。

我的理解是，就是一层层网络中每个节点的权重从一个训练好的网络迁移到一个全新的网络里，而不是从头开始。

例：牛津的VGG模型、谷歌的Inception模型和word2vec模型、微软的ResNet模型。

特点：需求的训练数据集合较小、训练时间较小、可以方便的进行迁移以满足个性化。

# 线性回归
## 什么是机器学习
对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。
- 什么是任务T 性能度量P 经验E
    - 任务T ：可以是输入和输出
    - 性能度量P：正向度量，预测值和真实值之间的差值
    - 经验E: 训练集，数据
- 线性回归model 形如 `$Y = W^T + b$` ` matmul(x, w) + b` 

## ML 的过程
### 过拟合
过拟合（overfitting）是指学习时选择的模型所包含的参数过多（即模型容量很大），以至于出现这一模型对已知数据预测得很好，但对未知数据预测得很差的现象。

当我们增加容量时，训练误差减小，但是训练误差和泛化误差之间的间距却不断增大，最终，这个间距的大小超过了训练误差的下降，我们进入到了**过拟合机制**

![image](https://note.youdao.com/yws/res/34602/9EED98F10019434289809ECB34E02140)

高偏置 低方差  一般意味着欠拟合

低偏置 高方差  一般意味着过拟合


#### 产生过拟合的原因
##### 数据有噪声
为什么数据有噪声，就可能导致模型出现过拟合现象呢？这是因为，当噪声数量在训练集中占有相当大的比例时，就会与正常数据一起影响训练集的分布，机器学习算法在学习过程中，把正常数据和噪声数据同时拟合，学习到了一个正常数据与噪声数据共同决定的模型，用此模型去预测从正常数据分布上取的未知数据，就很有可能得到不理想的泛化效果。
##### 训练数据不足，有限的训练数据
即使得到的训练数据没有噪声，当我们训练数据不足的时候，训练出来的模型也可能产生过拟合现象。这是因为训练数据有限，无法体现数据整体的分布。用给定的不充分的数据集上学习到的模型，去预测未知数据集上的数据，很大可能产生过拟合现象。如给定数据仅能够拟合出一个线性模型，而训练集和测试集总体服从非线性分布，用线性模型去预测非线性分布的数据，泛化能力显然不好。
##### 训练模型过度导致模型非常复杂
在有噪声的训练数据中，我们要是训练过度，会让模型学习到噪声的特征，无疑是会造成在没有噪声的真实测试集上准确率下降。

#### 如何解决过拟合
##### Early stop（提前停止）
P151

提前停止就是在验证误差开始上升之前，就把网络的训练停止了，具体做法是每次在给定的迭代次数K内，把出现比之前验证误差更小的参数（模型）记录下来。如果从上次记录最小值开始，迭代了K次仍然没有发现新的更小的验证误差，那就认为已经过拟合，把当前验证误差最小的模型当做最优模型。
##### Data expending（数据集扩增）
P148

过拟合是因为模型学习到了训练集的独有特性，那么如果我们的训练集能够涵盖所有样本空间，那么它就会学习到样本特征空间的普遍特性，而独特性将不复存在，因为测试集不会超出样本的特征空间，所以结果和训练集应该一致。
##### regularization（正则化）
P142

正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。

正则化可能会增加biass
##### DroupOut（随机失活）
P159

假设我们选取一个隐层，在每一次梯度迭代中，让隐层的每一个单元以一定的概率失活，这相当于训练一个新的子网络。下一次迭代继续上述过程。Dropout的底层思想其实也是把多个模型的结果放到一起，只不过实现方式是在一个模型内部，而不是像Bagging一样，真正地去训练多个模型。定性地理解一下，dropout实现的相当于每次迭代都在一个随机挑选的特征子空间进行。在实现dropout的时候，因为训**练时对单元按照一定概率p进行随机置零，所以训练结束后到了预测阶段需要对参数的值乘上一个1-p**，才能保证结果的正确性。

### 欠拟合
欠拟合就是模型没有很好地捕捉到数据特征，不能够很好地拟合数据。

训练误差和泛化误差都非常大

欠拟合：是指模型的学习能力比较低，以至于，只学到了很少的一部分信息，当进行预测的时候，会发生方差最低，和偏差较大的情况 
#### 如何解决欠拟合
1. 选择合适的损失函数；
2. 尝试采用Mini-batch以及Batch Norm的方法进行优化；
3. 选用其他激活函数；
4. 采用自适应学习率的优化算法；
5. 进行优化时考虑Momentum。

### 如何判断
#### 三种误差的计算方法
##### training error
![image](https://note.youdao.com/yws/res/34622/8E61E823156F4363BC7AA62417F15CA5)
##### cross validation error
![image](https://note.youdao.com/yws/res/34621/82D71C84DA9D4F34A3EB8BF90725E4B8)
##### test error
![image](https://note.youdao.com/yws/res/34620/2FB51590F08744D690B9F6364135BC11)

#### 学习曲线（learning curves）
学习曲线就是比较 j_train 和 j_cv。如下图所示，为一般的学习曲线，蓝色的线表示训练集上的误差 j_train, 粉色的线表示验证集上的误差 j_cv，横轴表示训练集合的大小。

![image](https://note.youdao.com/yws/res/34615/791591DBBB60458785F53873D732F93C)

刚开始处于 “A” 点，表示当训练数据很小时，很容易时训练集上的误差非常小，此时处于过拟合状态。随着训练数据的增加，训练数据上的误差 J_train 越来越大，而验证集上的误差 J_cv 越来越小，J_train 和 J_cv 越来越接近但始终保持 J_cv > J_train.

#### 交叉验证（cross-validation）
这里首先解释一下bias和variance的概念。模型的Error = Bias + Variance，Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。

![image](https://note.youdao.com/yws/res/34600/148B0FDE7A414ACD91CCE9B6344CF3FF)

当观察到 J_cv 很大时，可能处在途中蓝色圆圈中的两个位置，虽然观察到的现象很相似(J_cv都很大)，但这两个位置的状态是非常不同的，处理方法也完全不同。

- 当cross validation error (Jcv) 跟training error(Jtrain)差不多，且Jtrain较大时，即图中标出的bias，此时 high bias low variance，当前模型更可能存在欠拟合。
- 当Jcv >> Jtrain且Jtrain较小时，即图中标出的variance时，此时 low bias high variance，当前模型更可能存在过拟合**。**

## Gradient Descent
![image](https://note.youdao.com/yws/res/34593/04EF36BE52864887A7C2DF80A5C61919)

上标是第几个样本，下标是第几个样本的第几个参数。图上每次更新两个参数，可以合起来写。

![image](https://note.youdao.com/yws/res/34582/76B61EF5642D4EAAB7EFA7696036B5A4)

走的是Gradient方向的法线方向。

## 误差来源的分析
- Testing Error  = Bias Error + Variance Error


- Bias  与正确值之间的gap  
一般使用 训练集上面的错误率去衡量； 一般分为avoidable error + unavoidable error ，重点解决 avoidable error

![image](https://note.youdao.com/yws/res/34618/907C9141BDB944F7938F459FCA32FE3C)

- Variance  预测值的离散程度，一般认为是 开发集上的表现比训练集上的差多少,如果有比较多的样本，方差就会比较下，比较集中 开发集 - 训练集

![image](https://note.youdao.com/yws/res/34616/809844FF1CAE4195BDE9970E3A4A1B96)

![image](https://note.youdao.com/yws/res/34584/7E197BF1EE914584A914E46E55821F0B)

![image](https://note.youdao.com/yws/res/34592/4598850B5D43462395534EBB75610F44)

**模型越复杂，biase会低，但是Variance会高**。如果模型简单的话，就是说the best function 只能在这个简单的模型中去找，所以找到的function会biase小，但是可能不包含target，无论怎么找都不会找到自己想要的best function。

但是，如果模型比较复杂，可能就包含target，虽每次的结果不一样但是有可能就在target周围。

![image](https://note.youdao.com/yws/res/34566/2B3880CD1B8D4EA2876CE218A5EE6457)

模型阶数越大越复杂，bias低，variance大。同时考虑bias和variance，就是蓝色的线，然后就是下面的过拟合和欠拟合。

- 欠拟合： 高 bias error ,低variance error；都不能fit training data，预测值和真实值差很多，没有训练好。
    - 要做的是： 设计一个更新复杂的模型；增加输入特征

![image](https://note.youdao.com/yws/res/34613/B7DFCB2A60314387AC45109700F4E91D)

- 过拟合：  低bias error, 高 variance error ： 开发集上表现的比较差，训练集上面表现的很好，variance error 比较大
    - 要做的是：增加数据集。大数定理
![image](https://note.youdao.com/yws/res/34596/A3E933F638554B6B96D85B8B1B56C1A7)
    - 正则化：可以让曲线更加平滑，但是调整了funtion，可能会伤害bias，所以需要在bias和variance之间找一个平衡。
![image](https://note.youdao.com/yws/res/34595/11BE757CA2D246DEBC11C2AE89D932F6)


## 交叉验证


将Training Set 分为 training set 和validation set，training set 用来训练 model，validation set 用来选model。

图上 training set 选出了三个model，并且在validation set 上计算error，这里选model 3。然后用全部的Training set 在Model 3 上再选择一次。然后放在public Testing Set 上，一般error会变高，但是不建议再回头，因为这样就考虑了再public上的performance，不是private上的。

**训练集和测试集要一样。不要把test set 暴露给model**。
- 训练集： 梯度下降学习参数
- 开发/验证集： 挑选超参数
- 测试集： 估计泛化误差
两层for循环

### N- 折交叉验证
一次CV的分training set是固定的，如果不相信一次CV的结果，那么就进行N-fold，进行不同的分法。一般用于数据集不是特别大时

![image](https://note.youdao.com/yws/res/34617/9F050D988D9C44988FE10CDEF7C7CC03)

拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，最后求平均误差

不断选择test 子文件，可以把train set 的数据进行训练

![image](https://note.youdao.com/yws/res/34579/82FF1111F9FC42C08507302B2E6E56C4)

选择一部分作为test fold，其他作为traing fold，分别计算不同分法的误差，然后去平均。



## 模型参数和超参数
- 参数：自动选择
- 超参数： 人工定义

## 正则化
### L1正则化  
向量元素绝对值之和，即：

![image](https://note.youdao.com/yws/res/34634/13BD03DF6B7040ADA5F0E2C080614E03)
    
### L2： 
比L1 约束的要小一点,各个元素的平方和，即：

![image](https://note.youdao.com/yws/res/34625/00BCCD99AEF540CBA9BB4CE6A5E84546)

### L1 + L2

![image](https://note.youdao.com/yws/res/34626/9A104F9E44714279A9D040342848192F)

## 如何加快模型训练
### 数据归一化/标准化

### 梯度下降的变体 BGD SGD MBGD


### 改变学习率，用更大的步长去接近最佳值

## 梯度下降的变体
SGD BGD MBGD RMSpro Adam Adagrd

![image](https://note.youdao.com/yws/res/34611/DDD41D51472B4FD482AEB24EDA3A0571)



### 激活函数
#### 那什么是激活函数呢？

我们知道在多层的神经网络中，上一层的信号（也就是wx+b算出的结果）要作为下一层的输入，但是这个上一层的信号在输入到下一层之前需要一次激活f = sigmoid(wx+b)，因为并不是所有的上一层信号都可以激活下一层，如果所有的上一层信号都可以激活下一层，那么这一层相当于什么都没有做。因此我需要选择一些信号激活下一层的神经元。

如何表示激活呢？就是当activation function的输出结果是0，就代表抑制，是1，就代表激活。因为按照下一层的神经元的计算wx+b来看，如果x接近于0，那么就会阻止x对应的权重起作用，如果接近1，这个权重的作用会最大，在这个就是激活的含义。

#### Sigmoid
![image](https://note.youdao.com/yws/res/34630/69111540146B4C6893E2AAD0163B7A7F)

当使用Softmax进行二分类时，Softmax就退化成了Sigmiod，

![image](https://note.youdao.com/yws/res/34612/7694785FA661480B95251EB086298874)

#### Softmax
![image](https://note.youdao.com/yws/res/34628/D53F67F47CA04FA68ECBE4F59F8ED412)

Softmax Function，或称归一化指数函数，是逻辑函数的一种推广。它能将一个含任意实数的K维向量z “压缩”到另一个K维实向量 σ(z)  中，使得每一个元素的范围都在(0,1) 之间，并且所有元素的和为1。该函数的形式通常按下面的式子给出：

对每一层神经元上的权值输入到Softmax中进行求比重。

![image](https://note.youdao.com/yws/res/34608/56E96E79556B451A995B8C4A345F7E30)

求得的结果可以使用交叉熵作为损失函数进行梯度下降，找到交叉熵最小的权值，去预测目标。

#### SGD

### Adaptive Learning Rates
在训练开始的时候，学习率可以大一点，快结束的时候，要达到目标后，可以小一点，收敛到最低点的地方。

#### AdaGrad
![image](https://note.youdao.com/yws/res/34587/EDBE262FD46548D093CBF4523C02A806)

我们可以看出该优化算法与普通的sgd算法差别就在于标黄的
哪部分，采取了累积平方梯度。

![image](https://note.youdao.com/yws/res/34601/F4DB7595289648FC90886E519A8EA43C)

![image](https://note.youdao.com/yws/res/34597/AE4EFA798C094089B1EA18176566149C)

这里可以发现取平均的动作会抵消掉。

**简单来讲，设置全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同**

具有损失最大偏导的参数相应的有一个快速下降的学习率，二具有小偏导的参数在学习率上有相对较小的下降。

**在参数空间中更为平缓的方向会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。。**

特点：

- 前期g_t较小的时候， regularizer较大，能够放大梯度
- 后期g_t较大的时候，regularizer较小，能够约束梯度
- 适合处理稀疏梯度

缺点：
- 由公式可以看出，仍依赖于人工设置一个全局学习率
- `$\eta$`设置过大的话，会使regularizer过于敏感，对梯度的调节太大
- 中后期，分母上梯度平方的累加将会越来越大，使`$gradient\to0$`，使得训练提前结束


##### Example
假设我们现在采用的优化算法是最普通的梯度下降法mini-batch。它的移动方向如下面蓝色所示：

![image](https://note.youdao.com/yws/res/34605/97A7F79FE03D4E9E82908FF529E9548D)

假设我们现在就只有两个参数w,b，我们从图中可以看到在b方向走的比较陡峭，这影响了优化速度。

而我们采取AdaGrad算法之后，我们在算法中使用了累积平方梯度r=:r + g.g。

**从上图可以看出在b方向上的梯度g要大于在w方向上的梯度**。

那么在下次计算更新的时候**，r是作为分母出现的，越大的反而更新越小，越小的值反而更新越大，**那么后面的更新则会像下面绿色线更新一样，明显就会好于蓝色更新曲线。

但是，如果是训练深度神经网络模型，从训练开始时积累梯度平方会导致学习率过早和过量的减小。

#### RMSpro
![image](https://note.youdao.com/yws/res/34589/64EB623FC9F841FBB81EB7D732F424CE)

Adagrad根据平方梯度的整个历史收缩学习率,可能使得学习率在达到这样的凸结构前就变得太小

**RMSProp算法修改AdaGrad以在非凸设定下效果更好,改变梯度积累为指数加权平均移动。增加了 超参数（衰减速率）`$\rho$`，保留的历史梯度的比率，用来控制平均移动的长度范围**。


特点：
- 其实RMSprop依然依赖于全局学习率
- RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间
- 适合处理非平稳目标 - 对于RNN效果很好

#### Adam

![image](https://note.youdao.com/yws/res/34583/79C1E5D045AB453190A0616290AA37C4)

Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：

![image](https://note.youdao.com/yws/res/34606/710424ED8EED491084F230F77E8D391A)

特点：

- 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
- 对内存需求较小
- 为不同的参数计算不同的自适应学习率
- 也适用于大多非凸优化 - 适用于大数据集和高维空间

#### Monmentum

momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：


 

## Epoch Iteration Batch_size
- Epoch：对整个数据集进行一次forward和backward过程，被称为一个Epoch；
-  Batch_size：一次forward/backward过程中的训练样例数，被称为Batch_size；
- Batch：使用训练集中一小部分样本对模型权重进行一次反向传播的参数更新，这一小部分样本被称为一个Batch；
-  Iteration：一个Epoch所包含Batch的个数，被称为Iteration。

> Example

假设我的训练集中总样本数为2048、Batch_size=128，那么我需要迭代(Iterations)16次才能完成一个Epoch。


# 分类问题

1. Why not use linear regression model to solve classification?

- 直接使用线性回归的输出作为概率是有问题的，因为其值有可能小于0或者大于1,这是不符合实际情况的，逻辑回归的输出正是[0,1]区间。
- 线性回归中使用的是最小化平方误差损失函数，对偏离真实值越远的数据惩罚越严重。这样做会有什么问题呢？假如使用线性回归对{0,1}二分类问题做预测，则一个真值为1的样本，其预测值为50，那么将会对其产生很大的惩罚，这也和实际情况不符合，更大的预测值说明为1的可能性越大，而不应该惩罚的越严重。
- 逻辑回归使用对数似然函数进行参数估计，使用交叉熵作为损失函数，对预测错误的惩罚是随着输出的增大，逐渐逼近一个常数，这就不存在上述问题了


2. Why use cross entropy as loss metric, not MSE (Mean Square Error)?


3. What is Cascading logistic regression model? Why introduce this concept?




## 逻辑回归和线性回归的对比
- 逻辑回归 ： 增加激活函数 sigmoid 和softmax 实现二分类和多分类； 损失函数使用CE
- 线性回归： MSE  梯度下降基本一样

## 多分类和二分类问题
![image](https://note.youdao.com/yws/res/34624/0CC1A3412E064E719161566E1C280C76)

# Deep Learning
- 与机器学习训练的三部曲有一个不同：
- 这里的 Model 指的是一个网络结构

## 会算前向传播的过程
### 全连接前向传播的计算
>计算`$[1, -1]$`的输出
以下是网络结构图

![image](https://note.youdao.com/yws/res/34614/9AA894E05D0C4AFE82016EAE6DFA7F76)

![image](https://note.youdao.com/yws/res/34573/429C1ECA56264DA5A5F5C59CCF2FAF01)

> 另一个例子
![image](https://note.youdao.com/yws/res/34588/B8A0E9208B1345F1B3C2F052F1C34F58)

![image](https://note.youdao.com/yws/res/34591/87FDD055C2704728B3F1CBFB0BA88F37)

## 反向传播的计算
![image](https://note.youdao.com/yws/res/34604/7FDD77C076874245B6F1C1B62AEA34FC)

> Example

![image](https://note.youdao.com/yws/res/34574/F07E18336EA3429D8BA04E384851497A)

![image](https://note.youdao.com/yws/res/34577/9A98656F5F224DA6942A3AB0A3FAFD4F)

> 公式的推导

![image](https://note.youdao.com/yws/res/34578/159CB0CEB41F441B804B178D5CFBE43D)

![image](https://note.youdao.com/yws/res/34575/6BB49D5683864E4592348DAA70839256)

## 梯度消失和梯度爆炸
### 梯度消失
在多层网络中，影响梯度大小的因素主要有两个：权重和激活函数的偏导。深层的梯度是多个激活函数偏导乘积的形式来计算，如果这些激活函数的偏导比较小（小于1）或者为0，那么梯度随时间很容易vanishing。

反向传播时，需要计算偏导数，如果激活函数是Sigmoid函数，对其求导后，发现Sigmoid函数的导数最大也就0.25（当input=0时），而且很多情况input不会为0的，所以Sigmoid的导数会更小。那么计算靠近输入层参数的偏导数，难免会乘上几次Sigmoid函数的偏导，梯度就这样消失了。
#### 解决方法
设置记忆单元记住当前的信息

### 梯度爆炸
和梯度消失一样，如果这些激活函数的偏导比较大（大于1），那么梯度很有可能就会exploding。这些大于1的偏导会导致靠近输入层的参数变化比较快，靠近输出层的参数相较而言变化慢，导致梯度爆炸的问题。
#### 解决方法
1. 重新设计网络模型，减少网络层数有效解决梯度不稳定问题；
2. 使用 ReLU 激活函数，ReLU求完微分后不会引起梯度消失或爆炸的问题，而且计算速度快，加速了网络的训练；
3. 使用LSTM，LSTM单元和相关的门类型神经元结构可以减少梯度消失问题；
4. 使用梯度截断，自定一个阈值，梯度再大也不能超过这个阈值；
5. Batch-Norm，Batch-Norm通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题；
6. 残差网络结构，残差可以很轻松的构建几百层，一千多层的网络而不用担心梯度消失过快的问题，原因就在于残差的捷径（shortcut）部分。

## CNN 和 RNN
### CNN
卷积网络的核心思想是将：**局部感受野、权值共享以及时间或空间亚采样**这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。

![image](https://note.youdao.com/yws/res/34562/E10C5B45F28146658028C3B06F842AE3)

使用filter进行卷积，图上就是找与filter类似的，这里可以发现对角线方向都是1的值是最大的，就是与filter类似的。
#### CNN与FC的关系
![image](https://note.youdao.com/yws/res/34563/544465D580004054AE3550E076D44AC6)

![image](https://note.youdao.com/yws/res/34567/F2AE83350F57431BBB978139B9E0D085)

图上是CNN与FC关系，先将这个image展开也就是36 * 1，如果是全连接，就要与这36个权值连接；如果是卷积，就是与中间九个（也就是卷积核的大小）连接，将这个九个展开拉直，就是全连接时的第1， 2， 3， 7， 8， 9，13，14，15九个，可以看到不同的颜色对应filter的不同的权值。但是用的filter是共用的.

简单的说就是两个特点：**稀疏连接 + 权值共享**

#### CNN 流程
 看CNN 笔记
卷积神经网络的层级结构

- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层 / ReLU layer
- 池化层 / Pooling layer
- 全连接层 / FC layer
 
![image](https://note.youdao.com/yws/res/34570/32DB4F6E214444AFAAF0C7C381B1D764)
  
##### 数据输入层
该层要做的处理主要是对原始图像数据进行预处理，其中包括：

　　• 去均值：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。

　　• 归一化：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。

　　• PCA/白化：用PCA降维；白化是对数据各个特征轴上的幅度归一化

去均值与归一化效果图：

![image](https://note.youdao.com/yws/res/34598/84BF93EBC8CD437098AB5EF6591491FA)

去相关与白化效果图：

![image](https://note.youdao.com/yws/res/34603/370A54A2DA4F4753B8B9683DFA58F382)

##### 卷积计算层
这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。

在这个卷积层，有两个关键操作：
- 局部关联。每个神经元看做一个滤波器(filter)
- 窗口(receptive field)滑动， filter对局部数据计算

卷积层遇到的几个名词：
- 步长/stride （窗口一次滑动的长度）
- 填充值/zero-padding
    - 由于每次计算的结果都是存在选出来的中心位置的元素块，这样的话原Feature的边界的位置无法计算，所以通常可以在原Feature的四周加上一圈元素为1 或 -1.

**卷积的计算**（下面蓝色矩阵周围有一圈灰色的框，那些就是上面所说到的填充值）

这里的蓝色矩阵就是输入的图像，粉色矩阵就是卷积层的神经元，这里表示了有两个神经元（w0,w1）。绿色矩阵就是经过卷积运算后的输出矩阵，这里的步长设置为2。

![image](https://note.youdao.com/yws/res/34635/78F64175817E4894A870549D0EBD2FB0)

蓝色的矩阵(输入图像)对粉色的矩阵（filter）进行矩阵内积计算并将三个内积运算的结果与偏置值b相加（比如上面图的计算：2+（-2+1-2）+（1-2-2） + 1= 2 - 3 - 3 + 1 = -3），计算后的值就是绿框矩阵的一个元素。

##### 激励层ReLU
把卷积层输出结果做非线性映射。

CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。

![image](https://note.youdao.com/yws/res/34610/95DEB1F7DC994EB089D6B5CF766BD257)

注意：
- 不要用 sigmoid；如果>1,层数多容易梯度爆炸，如果<1,层数多容易梯度消失
- 也可以使用Maxout tanh

##### 池化层
池化层夹在连续的卷积层中间，用于压缩数据和参数的量，减小过拟合。常用的是Max pooling和 average pooling，上面的是Max pooling, 在CNN笔记中使用的是average pooling。

这里使用的是max pooling。

![image](https://note.youdao.com/yws/res/34599/D56BBF0D0F9F4F0285A12102E47A3E7E)

对于每个2 * 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个 2 * 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。

**池化层的作用**：
1. **特征不变性**，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。
2. **特征降维**，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。
3. 在一定程度上**防止过拟合**，更方便优化。

##### 全连接层
两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的：

![image](https://note.youdao.com/yws/res/34586/79416529E02C41BB90C1189BBBFF35C5)

tf 的部分代码：
```
 w_c1 = tf.Variable(w_alpha * tf.random_normal([3, 3, 1, 32])) # filter
    b_c1 = tf.Variable(b_alpha * tf.random_normal([32]))
    conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, w_c1, strides=[1, 1, 1, 1], padding='SAME'),
```
```
conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None,data_format=None, name=None):

padding = 'SAME' or 'VALID'分别表示 零填充和不填充。
```
##### CNN特点
卷积网络在本质上是一种输入到输出的映射，它能够学习大量的输入与输出之间的映射关系，而**不需要任何输入和输出之间的精确的数学表达式**，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。

CNN一个非常重要的特点就是**头重脚轻**（越往输入权值越小，越往输出权值越多），呈现出一个倒三角的形态，这就很好地避免了BP神经网络中反向传播的时候梯度损失得太快。

卷积神经网络CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，**避免了显式的特征抽取**，而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元**权值相同**，所以**网络可以并行学习**，这也是卷积网络相对于神经元彼此相连网络的一大优势。卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，****权值共享降低了网络的复杂性****，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。


### RNN

具有短期记忆能力的神经网络，在网络中会有存储，记忆当前的状态，一般记忆中的初始值是0.
![image](https://note.youdao.com/yws/res/34580/F183E00993104145AF0B7F940E347C1F)

 > Example
 
 1. 假设所有的weights是1，没有bias，激活函数时线性的，输入是1的话，那么输出是4.
 
 ![image](https://note.youdao.com/yws/res/34569/3CE94FACC6164590BCF05558B7FCED1C)

2. 中间一层的输出是2，并且将它存储，那么两个记忆单元的值就是2，并且用于下一次。那么中间一层的下一次的结果就是 1 * 1 + 1 * 1+ 2 * 2 + 2 * 2 = 6，并记忆，最后的输出就是12.

![image](https://note.youdao.com/yws/res/34572/1D76A0C4B39F4CAA9B160B93BEE07BDF)

3. 假设这次的输入是[2， 2]记忆单元开始使用中间层的6 ，绿色的计算出 16 ，红色的就是32.

![image](https://note.youdao.com/yws/res/34568/E08B69528CC74B74B52469B908291796)

注意： 以上输入序列的三个输入（[1,1],[1,1],[2,2]）这个顺序是不能变的，如果变了结果就会不一样。

上面的输入在这里就变成了sentence，每一个输入就是一个word，同样的，这里的词序也不能变。要注意的是，这里虽然画了三个，但是是同一个网络，只是在不同的时刻进行的。
![image](https://note.youdao.com/yws/res/34565/768EBBF2FCB542A7AA2EC1CABDD1C26D)

### LSTM(Long Short-term Mermory)
三个门控制。可以看成一个特殊的神经元，四个输入，一个输出。

tips： 这个dash 应该放在 Short 和term 之间，因为这个还是一个short -term的神经网络。只是可以long一点，只要forget gate选择记忆。

![image](https://note.youdao.com/yws/res/34585/4C8A56DE53A54BE98C80F7CDA17D81F4)

#### LSTM的过程
1. 初始状态

![image](https://note.youdao.com/yws/res/34609/5A8A634276074DDBB9589159C3146BAC)

2. 工作过程
- 设输入的是`$z$`，output 分别是由`$z_i$`,`$z_f$`,`$z_o$`操控，`$z$`先通过一个activative function（sigmoid）得到g(z)，
- `$z_i$`通过activate function得到`$f(z_i)$`,然后相乘得到`$g(z)f(z_i)$`。
- `$z_f$`通过activate function得到`$f(z_f)$`,然后让记忆单元里的`$c$`与`$f(z_f)$`相乘得到`$Cf(z_f)$`，然后加上`$g(z)f(z_i)$`，得到`$c' = g(z)f(z_i) + cf(z_f)$` ,`$c'$`就是新的存在mermory里面的值。
- 然后将`$c'$`通过一个激活函数得到`$h(c')= h(g(z)f(z_i) + cf(z_f))$`
- 最后将`$z_o$`通过activate function得到`$f(z_o)$`,`$h(c')$`通过output gate，也就是与`$f(z_o)$`相乘，得到`$h(c')f(z_o) = h(g(z)f(z_i) + cf(z_f)) * f(z_o)$`，这就是这一次循环的最后的输出。

![image](https://note.youdao.com/yws/res/34571/FF7F25F86270451294BC718923705108)

tips： sigmoid的结果是(0, 1),得到的结果就是f(`$z_i$`)，f(`$z_f$`)，f(`$z_o$`),这些都会与输入相乘，为0就忘记，为1就记忆。
#### Example
这里有五个输入[3,1,0],[4,1,0],[2,0,0],[1,0,1],[3.-1,0],有四组权值()，对应的输出y是[0, 0, 0, 7, 0]，以下是结构图：
- 黄色的节点代表输入向量`$X$`的各个维度，绿色的节点节点代表的是bias`$b$`，边上的数字代表向量`$X$`权值`$W$`的各个维度
- 图中 ![image](https://note.youdao.com/yws/res/34631/E94693E9441A42D4A7022C216903AC2B)表示非线性变换函数`$f(x)$`，这里假设是sigmoid函数;![image](https://note.youdao.com/yws/res/34627/593D509947E940809B5432318BE6F4AE)与![image](https://note.youdao.com/yws/res/34629/B70C8D3970114EB08F06072DD88C80E9)表示线性变换函数`$g(x)$`与`$h(x)$`
- ![image](https://note.youdao.com/yws/res/34632/768EEDE77A1C417A97DAF35C601AB952)表示求和，即`$WX + b$`
- ![image](https://note.youdao.com/yws/res/34633/EA11DED215FA41F6975A0E09A4E324B3)表示求积
- ![image](https://note.youdao.com/yws/res/34623/36E364F9BA1B44F3B3AED7148A5084BC) 表示cell中的c的值

![image](https://note.youdao.com/yws/res/34581/40C1E89BA26040898640C58154796C50)

计算[3,1,0],[4,1,0]的输出过程：

![image](https://note.youdao.com/yws/res/34576/2E9F9C514D504802BD1E4073E84AB70F)

> 1. 输入`$X$` = [3, 1, 0]，输出为 0
```
// 下划线_,表示下标 
z = 1 * 3 +  0 * 1 +  0 * 1 + 0 * 1= 3 // w_1 = 1, w_2 = 0, w_3 = 0, w_b = 0; x_1 = 3 ，x_2 = 1, x_3 = 0; b = 1
g(z) = 3  //在这里简化g(z)为一个线性变换函数，假设是等量变换
z_i = 0 * 3  + 100 * 1 + + 0 * 0 + (-10) * 1 = 90 //权值共享 z_i = z
f(z_i) = f(90) ≈ 1 // f(z_i) 表示sigmoid变换，作为input gate
g(z) * f(z_i) = 3
此时cell中为初始值，C = 0
z_f = 0 * 3 + 100 * 1 + 0 * 0 + 10 * 1 = 110
f(z_f) = 1 //forget gate
c' = C * f(z_f) + g(z) * f(z_i) = 0 * 1 + 3 = 3 
h(c') = 3 //在这里简化h(z)为一个线性变换函数，假设是等量变换
z_o = 0 * 4 + 0 * 1 + 100 * 0 + (-10) * 1 = -10
f(z_o) = 0 //output gate
h(c')f(z_o) = 0 //最终的输出
```
> 2. 输入`$X$` = [4, 1, 0]，输出为 0
```
z = 1 * 4 +  0 * 1 +  0 * 1 + 0 * 1= 4 
g(z) = 4
z_i = 0 * 4 + 100 * 1 + 0 * 0 + (-10) * 1 = 90 
f(z_i) = f(90) ≈ 1 
g(z) * f(z_i) = 4
此时cell中为上一步的c'的值，C = 3
z_f = 0 * 4 + 100 * 1 + 0 * 0 + 10 * 1 = 110
f(z_f) = 1 
c' = C * f(z_f) + g(z) * f(z_i) = 3 * 1 + 4 = 7 
h(c') = 7 
z_o = 0 * 4 + 0 * 1 + 100 * 0 + (-10) * 1 = -10
f(z_o) ≈ 0 
h(c')f(z_o) = 0 
```
> 3. 输入`$X$` = [2，0, 0]，输出为 0
```
z = 1 * 2 +  0 * 0 +  0 * 0 + 0 * 1= 2
g(z) = 2
z_i = 0 * 2 + 100 * 0 + 0 * 0 + (-10) * 1 = -10
f(z_i) = f(-10) ≈ 0 
g(z) * f(z_i) = 0
此时cell中为上一步的c'的值，C = 7
z_f = 0 * 2 + 100 * 0 + 0 * 0 + 10 * 1 = 10
f(z_f) = 1 
c' = C * f(z_f) + g(z) * f(z_i) = 7 * 1 + 4 = 11
h(c') = 11 
z_o = 0 * 2 + 0 * 0 + 100 * 0 + (-10) * 1 = -10
f(z_o) ≈ 0 
h(c')f(z_o) = 0 
```
同理可以得
> 4.输入`$X$` = [1, 0, 1]，输出为 7

> 5.输入`$X$` = [3. -1, 0]， 输出为 0






#### LSTM与普通的network的联系
可以将LSTM的那个memory cell想象成一个neuron，如果要使用LSTM network, 可以将原来的network中的neuron 换成一个LSTM的 cell，要注意的是，转换后，四个input才有一个output，需要的参数量是原来的四倍。 如下：

原来的network：

![image](https://note.youdao.com/yws/res/34590/B103B197E4F44598940A775752CF4C45)

换为LSTM的 cell，注意这里的四个input都一样，可以共享输入。

![image](https://note.youdao.com/yws/res/34560/ADFDDDBCBCA54CE393E6D67C50B79F52)

#### LSTM与RNN的联系
**long time dependence**

两句话，was 还是were是根据第二个单词决定的。 但是在简单的RNN 神经网络中，距离越远，相互之间的联系和影响就会越弱，通过增加记忆单元解决。

每一个 cell 里存的是一个scalar，把所用的scalar 连接起来就变成了一个vector，设为`$c^{t-1}$`。在t时刻输入一个vector，`$x^t$`,经过线性变换（乘上一个矩阵）后变为z，然后将z的每一个dimension代表着操控LSTM每一个cell的input，所以z的dimension就是cell的数目。同理有操控的input gate，output gate,forget gate  的`$z^i$`,`$z^o$`,`$z^f$`,

![image](https://note.youdao.com/yws/res/34594/6B3F0F29774B4F2085F06D5C85CF4274)

所有的cell可以通过一起被运算, 假设一开始cell里的是`$c^{t-1}$` `$z^i$`经过sigmoid后与z做内积，同时`$z^f$`经过sigmoid后与`$c^{t-1}$` 相乘，这两个结果相加得到`$c^t$`并保存到cell中，再把相加后的结果经过sigmoid后与`$z^o$`经过sigmoid后相乘，就是输出`$y^t$`. 下一次的就是`$c^{t-1}$`

![image](https://note.youdao.com/yws/res/34564/AB8AFC09B5AE4356A64640F1E6FA7139)

更复杂的是，`$c^{t-1}$`,`$h^{t-1}$`,也会作为下一次的输入。

![image](https://note.youdao.com/yws/res/34561/6F21E7C27A474664B320F3AA3CC4AECD)

#### RNN的梯度消失与梯度爆炸
- RNN 不容易训练，层数很多有time senquence，会被反复训练，w任何一点点的改变都会对结果有很大的影响。
    - 使用LSTM 解决梯度消失（注意不是梯度爆炸）,在LSTM中，input的影响会永远存储在cell中，因为有之前的信息，所以不会有梯度消失问题；所以，forget gate要保证大部分的都是开启的，选择记住，因为要保存大部梯度有效信息，防止梯度消失。
    
- 序列数据可以考虑RNN； 语音，视频，翻译
- 即使只用一层RNN，仍可能梯度爆炸啊，有循环
- LSTM 与RNN 比较，优势？
解决梯度爆炸，记忆单元门

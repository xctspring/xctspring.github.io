---
layout: post
title: LSTM和VGG
date: 2019-12-26
categories: AI
tags: AI
---

## LSTM(Long Short-term Mermory)
LSTM是一个特殊的RNN，所以先简单了解一下RNN的结构。
### RNN

- 最大的特点就是有一个循环。
- 在每一个时间步中，RNN传递一个激活值到下一个时间步中用于计算。也就是说，具有短期记忆能力的神经网络，在网络中会有存储，记忆当前的状态，一般记忆中的初始值是0.
![image](https://note.youdao.com/yws/res/34580/F183E00993104145AF0B7F940E347C1F)

以下是整体的RNN结构图。

![image](https://note.youdao.com/yws/res/8126/B1F977F1BC15471A901C72C3C9AA7CC4)
- `$a^{<t>}$`:激活值，记忆上一步的神经元的信息，一般初始化为全零向量。
- `$x^{<1>}……x^{<t>}……x^{<T_x>}$` 每一步的输入向量
- `$y^{<t>}$` 每一步输出的预测向量
- `$W_{aa}$` 计算`$a^{<t>}$`的权值，控制是激活值`$a^{<t>}$`在时间步数之间的水平联系，这个是共享的
- `$W_{ax}$` 计算`$x^{<1>}$`到隐藏层的一系列权值。
- `$W_{ay}$` 权值，决定输出层的输出结果。

 > Example
 
 1. 假设所有的weights是1，没有bias，激活函数是线性的等量变换，输入是[1,1]的话，那么两个绿色节点为1 * 1  + 1 * 1 = 2，最终的输出是2 * 1 + 2 * 1 = 4, 即[4,4].
 ![image](https://note.youdao.com/yws/res/34936/C02487F80DC94C6984955275BDB80FC2)

2. 绿色节点在上一层被记忆，那么两个记忆单元的值就是2，并且用于下一次。那么中间一层的下一次的结果就是 1 * 1 + 1 * 1+ 2 * 2 + 2 * 2 = 6，并记忆，最后的输出就是12.

![image](https://note.youdao.com/yws/res/34572/1D76A0C4B39F4CAA9B160B93BEE07BDF)

3. 假设这次的输入是[2， 2]记忆单元开始使用中间层的6 ，绿色的计算出 16 ，红色的就是32.

![image](https://note.youdao.com/yws/res/34568/E08B69528CC74B74B52469B908291796)

- 注意到以上输入序列的三个输入（[1,1],[1,1],[2,2]）这个顺序是不能变的，如果变了结果就会不一样，所以RNN是一个时间序列的神经网络，接收的输入应该是语音、视频、文本等有时间序的输入。

### 特殊的RNN
RNN有着以下缺点：
- 长期依赖问题：每一次都记录上一步的状态，距离越远，相互之间的影响和联系就会越弱
- 梯度消失或者爆炸问题：由于是循环网络，内部是乘积运算，如果乘的小就会梯度消失，如果大就会爆炸。所以RNN不方便训练，W的微小变化就会对结果产生很大的影响。

LSTM提出了门机制解决这些问题，所以LSTM是一个特殊的RNN
- 门机制就是一个函数变换，一般为sigmoid，利用它非0即1的特点，实现门控制
- 一共三个门控制。
    - input gate 控制是否接收本层的输入
    - forget gate 控制是否记忆本层的状态
    - output gate 控制是否输出本层的输出
- 可以看成一个特殊的神经元，四个输入(可以共享一个相同的输入向量)，一个输出。

![image](https://note.youdao.com/yws/res/34585/4C8A56DE53A54BE98C80F7CDA17D81F4)

### LSTM的工作过程
![image](https://note.youdao.com/yws/res/34571/FF7F25F86270451294BC718923705108)
- 设输入的是`$z$`，output 分别是由`$z_i$`,`$z_f$`,`$z_o$`操控，`$z$`先通过一个activative function（sigmoid）得到g(z)，
- `$z_i$`通过activate function得到`$f(z_i)$`,然后相乘得到`$g(z)f(z_i)$`。
- `$z_f$`通过activate function得到`$f(z_f)$`,然后让记忆单元里的`$C$`与`$f(z_f)$`相乘得到`$Cf(z_f)$`，再加上`$g(z)f(z_i)$`，得到`$c' = g(z)f(z_i) + Cf(z_f)$` ,`$c'$`就是新的存在mermory cell里面的值。
- `$c'$`通过一个激活函数得到`$h(c')= h(g(z)f(z_i) + Cf(z_f))$`
- `$z_o$`通过activate function得到`$f(z_o)$`,`$h(c')$`通过output gate，也就是与`$f(z_o)$`相乘，得到`$h(c')f(z_o) = h(g(z)f(z_i) + cf(z_f)) * f(z_o)$`，这就是当前时间步数的输出。


tips： sigmoid的结果是(0, 1),就是`$f(z_i)$`，`$f(z_f)$`，`$f(z_o)$`,这些都会与输入相乘，为0就忘记，为1就记忆。
#### Example
最好自己算一遍。

这里有五个输入[3, 1, 0], [4 ,1 ,0], [2, 0, 0], [1 ,0 , 1], [3, -1, 0]，对应的输出y是[0, 0, 0, 7, 0]，以下是结构图：
![image](https://note.youdao.com/yws/res/34581/40C1E89BA26040898640C58154796C50)
- 黄色的节点代表输入向量`$X$`的各个维度，绿色的节点节点代表的是bias`$b$`，边上的数字代表向量`$X$`权值`$W$`的各个维度
- 图中 ![image](https://note.youdao.com/yws/res/34631/E94693E9441A42D4A7022C216903AC2B)表示非线性变换函数`$f(x)$`，这里假设是sigmoid函数;![image](https://note.youdao.com/yws/res/34627/593D509947E940809B5432318BE6F4AE)与![image](https://note.youdao.com/yws/res/34629/B70C8D3970114EB08F06072DD88C80E9)表示线性变换函数`$g(x)$`与`$h(x)$`
- ![image](https://note.youdao.com/yws/res/34632/768EEDE77A1C417A97DAF35C601AB952)表示求和，即`$WX + b$`
- ![image](https://note.youdao.com/yws/res/34633/EA11DED215FA41F6975A0E09A4E324B3)表示求积
- ![image](https://note.youdao.com/yws/res/34623/36E364F9BA1B44F3B3AED7148A5084BC) 表示cell中的c的值

具体的计算过程为：

> 1. 输入`$X$` = [3, 1, 0]，输出为 0
```
// 下划线_,表示下标 
z = 1 * 3 +  0 * 1 +  0 * 1 + 0 * 1= 3 // w_1 = 1, w_2 = 0, w_3 = 0, w_b = 0; x_1 = 3 ，x_2 = 1, x_3 = 0; b = 1
g(z) = 3  //在这里简化g(z)为一个线性变换函数，假设是等量变换
z_i = 0 * 3  + 100 * 1 + + 0 * 0 + (-10) * 1 = 90 //权值共享 z_i = z
f(z_i) = f(90) ≈ 1 // f(z_i) 表示sigmoid变换，作为input gate
g(z) * f(z_i) = 3
此时cell中为初始值，C = 0
z_f = 0 * 3 + 100 * 1 + 0 * 0 + 10 * 1 = 110
f(z_f) = 1 //forget gate
c' = C * f(z_f) + g(z) * f(z_i) = 0 * 1 + 3 = 3 
h(c') = 3 //在这里简化h(z)为一个线性变换函数，假设是等量变换
z_o = 0 * 4 + 0 * 1 + 100 * 0 + (-10) * 1 = -10
f(z_o) = 0 //output gate
h(c')f(z_o) = 0 //最终的输出
```
> 2. 输入`$X$` = [4, 1, 0]，输出为 0
```
z = 1 * 4 +  0 * 1 +  0 * 1 + 0 * 1= 4 
g(z) = 4
z_i = 0 * 4 + 100 * 1 + 0 * 0 + (-10) * 1 = 90 
f(z_i) = f(90) ≈ 1 
g(z) * f(z_i) = 4
此时cell中为上一步的c'的值，C = 3
z_f = 0 * 4 + 100 * 1 + 0 * 0 + 10 * 1 = 110
f(z_f) = 1 
c' = C * f(z_f) + g(z) * f(z_i) = 3 * 1 + 4 = 7 
h(c') = 7 
z_o = 0 * 4 + 0 * 1 + 100 * 0 + (-10) * 1 = -10
f(z_o) ≈ 0 
h(c')f(z_o) = 0 
```
> 3. 输入`$X$` = [2，0, 0]，输出为 0
```
z = 1 * 2 +  0 * 0 +  0 * 0 + 0 * 1= 2
g(z) = 2
z_i = 0 * 2 + 100 * 0 + 0 * 0 + (-10) * 1 = -10
f(z_i) = f(-10) ≈ 0 
g(z) * f(z_i) = 0
此时cell中为上一步的c'的值，C = 7
z_f = 0 * 2 + 100 * 0 + 0 * 0 + 10 * 1 = 10
f(z_f) = 1 
c' = C * f(z_f) + g(z) * f(z_i) = 7 * 1 + 4 = 11
h(c') = 11 
z_o = 0 * 2 + 0 * 0 + 100 * 0 + (-10) * 1 = -10
f(z_o) ≈ 0 
h(c')f(z_o) = 0 
```
同理可以得
> 4.输入`$X$` = [1, 0, 1]，输出为 7

> 5.输入`$X$` = [3. -1, 0]， 输出为 0

> 最后的输出为`$Y$` = [0, 0, 0, 7, 0]

## VGG
### VGG结构的基本理解
VGG是一个经典的CNN网络

以下是VGG的结构的简单示意图

![image](https://note.youdao.com/yws/res/35036/1CAF2017E5974B1991807E57BB96454B)

- 图中没有关于每一层的卷积核，stride，padding的说明，详细的说明可以查看 https://dgschwend.github.io/netscope/#/preset/vgg-16 
- 输出层是224x224x3 即图像宽高为224，剩下的三个通道对应RGB
- 网络总共的层数为16层，13层卷积，3层全连接层。
    - 这里的13层卷积的特点就是每层的卷积核都是(3,3)的
    - 每层卷积核的个数不同，依次是
        - 两层64个的卷积核 
        - 两层128个的卷积核
        - 三层256个的卷积核
        - 六层512个的卷积核
    - 以上是VGG-16的结构，如果是VGG-19，就再加上一层256个的卷积核和两层512个的卷积核

下面选择前三层作简单分析
```
第一层：conv1_1
输入为 224×224×3的图像，卷积核的数量为64，卷积核的大小kernel_size为 3x3 ，stride = 2, stride表示的是步长， pad = 1, 表示边缘扩充1;
卷积后的图形大小是怎样的呢？
wide = (224 + 2 * padding - kernel_size) / stride + 1 = 224 //卷积核在图片上滑动卷积后的产生feature map的尺寸
height = (224 + 2 * padding - kernel_size) / stride + 1 = 224
dimention = 64
经过ReLU激活函数后，最后第一层输出的feature map 为(224, 224, 64)

第二层：conv1_2
输入为 224×224×3的图像，卷积核的数量为64，卷积核的大小为 3x3 ，stride = 2, pad = 1
wide = (224 + 2 * padding - kernel_size) / stride + 1 = 224
height = (224 + 2 * padding - kernel_size) / stride + 1 = 224
dimention = 64
经过ReLU激活函数后，最后第二层层输出的feature map 为(224, 224, 64)

然后会经过一次池化maxpooling
池化的pool_size = (2, 2) ,stride = 2
最后的输出为(112, 112, 64)
```
后面的原理一致， 可以依据 https://dgschwend.github.io/netscope/#/preset/vgg-16 的详细参数类推

### VGG 的特点
- 小卷积核和多卷积子层
    - VGG使用多个较小卷积核（3x3）的卷积层代替一个卷积核较大的卷积层，相当于进行了更多的非线性映射，可以增加网络的拟合/表达能力。 
- 通道数多
    - VGG网络第一层的通道数为64，后面每层都进行了翻倍，最多到512个通道，通道数的增加，使得更多的信息可以被提取出来
- VGG耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用，特别是三个全连接层。
- VGG相对其他的方法，参数空间很大，最终的model有500多m，AlexNet只有200m，GoogLeNet更少，所以train一个vgg模型通常要花费更长的时间
